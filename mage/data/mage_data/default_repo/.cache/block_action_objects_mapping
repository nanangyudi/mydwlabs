{"block_file": {"custom/duck_helpers.py:custom:python:duck helpers": {"content": "import duckdb\nimport pandas as pd\n\ndef upsert_to_duckdb(df: pd.DataFrame, path: str, table: str, key: str, order_col: str = 'write_date'):\n    if df is None or df.empty:\n        return  # no-op\n\n    con = duckdb.connect(path)\n    try:\n        df = df.copy()\n        # daftarkan DF sebagai view sementara\n        con.register('newdata', df)\n\n        # buat tabel kalau belum ada (skema diambil otomatis dari DF)\n        con.execute(f\"CREATE TABLE IF NOT EXISTS {table} AS SELECT * FROM newdata WHERE 1=0;\")\n\n        cols = [c for c in df.columns]\n        set_clause = ', '.join([f\"{c}=n.{c}\" for c in cols])\n        col_list  = ', '.join(cols)\n        val_list  = ', '.join([f\"n.{c}\" for c in cols])\n\n        # update hanya jika versi data baru >= lama (berdasar order_col)\n        on_clause = f\"t.{key} = n.{key}\"\n        matched_cond = f\"AND coalesce(n.{order_col}, TIMESTAMP '1970-01-01') >= coalesce(t.{order_col}, TIMESTAMP '1970-01-01')\"\n\n        con.execute(f\"\"\"\n            MERGE INTO {table} t\n            USING newdata n\n            ON {on_clause}\n            WHEN MATCHED {matched_cond} THEN UPDATE SET {set_clause}\n            WHEN NOT MATCHED THEN INSERT ({col_list}) VALUES ({val_list});\n        \"\"\")\n        con.unregister('newdata')\n    finally:\n        con.close()\n", "file_path": "custom/duck_helpers.py", "language": "python", "type": "custom", "uuid": "duck_helpers"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/x_dim_employee_to_duckdb.py:data_exporter:python:x dim employee to duckdb": {"content": "# requires: duckdb, pyarrow\nimport duckdb\nimport pandas as pd\nfrom mage_ai.data_preparation.decorators import data_exporter\nfrom mage_ai.data_preparation.variable_manager import get_global_variables\nfrom typing import Any, Dict\nfrom pathlib import Path\nfrom pandas import DataFrame\n\ndef upsert_to_duckdb(df: pd.DataFrame, path: str, table: str, key: str, order_col: str = 'write_date'):\n    if df is None or df.empty:\n        return\n    con = duckdb.connect(path)\n    try:\n        con.register('newdata', df)\n        con.execute(f\"CREATE TABLE IF NOT EXISTS {table} AS SELECT * FROM newdata WHERE 1=0;\")\n        cols = list(df.columns)\n        set_clause = ', '.join([f\"{c}=n.{c}\" for c in cols])\n        col_list  = ', '.join(cols)\n        val_list  = ', '.join([f\"n.{c}\" for c in cols])\n        on_clause = f\"t.{key}=n.{key}\"\n        matched_cond = f\"AND coalesce(n.{order_col}, TIMESTAMP '1970-01-01') >= coalesce(t.{order_col}, TIMESTAMP '1970-01-01')\"\n        con.execute(f\"\"\"\n            MERGE INTO {table} t\n            USING newdata n\n            ON {on_clause}\n            WHEN MATCHED {matched_cond} THEN UPDATE SET {set_clause}\n            WHEN NOT MATCHED THEN INSERT ({col_list}) VALUES ({val_list});\n        \"\"\")\n        con.unregister('newdata')\n    finally:\n        con.close()\n\n@data_exporter\ndef export_data(df: DataFrame, *args, **kwargs) -> None:\n    variables: Dict[str, Any] = get_global_variables(kwargs['pipeline_uuid'])\n    duckdb_path = variables.get('duckdb_path', '/home/src/duckdb/odoo_wh.duckdb')\n    Path(duckdb_path).parent.mkdir(parents=True, exist_ok=True)\n    upsert_to_duckdb(df, duckdb_path, table='dim_employee', key='employee_id', order_col='write_date')\n", "file_path": "data_exporters/x_dim_employee_to_duckdb.py", "language": "python", "type": "data_exporter", "uuid": "x_dim_employee_to_duckdb"}, "data_exporters/x_fact_attendance_to_duckdb.py:data_exporter:python:x fact attendance to duckdb": {"content": "# same helper as above\nimport duckdb, pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\nfrom mage_ai.data_preparation.decorators import data_exporter\nfrom mage_ai.data_preparation.variable_manager import get_global_variables\n\ndef upsert_to_duckdb(df: pd.DataFrame, path: str, table: str, key: str, order_col: str = 'write_date'):\n    if df is None or df.empty: return\n    con = duckdb.connect(path)\n    try:\n        con.register('newdata', df)\n        con.execute(f\"CREATE TABLE IF NOT EXISTS {table} AS SELECT * FROM newdata WHERE 1=0;\")\n        cols = list(df.columns)\n        set_clause = ', '.join([f\"{c}=n.{c}\" for c in cols])\n        col_list  = ', '.join(cols)\n        val_list  = ', '.join([f\"n.{c}\" for c in cols])\n        on_clause = f\"t.{key}=n.{key}\"\n        matched_cond = f\"AND coalesce(n.{order_col}, TIMESTAMP '1970-01-01') >= coalesce(t.{order_col}, TIMESTAMP '1970-01-01')\"\n        con.execute(f\"\"\"\n            MERGE INTO {table} t\n            USING newdata n\n            ON {on_clause}\n            WHEN MATCHED {matched_cond} THEN UPDATE SET {set_clause}\n            WHEN NOT MATCHED THEN INSERT ({col_list}) VALUES ({val_list});\n        \"\"\")\n        con.unregister('newdata')\n    finally:\n        con.close()\n\n@data_exporter\ndef export_data(df: DataFrame, *args, **kwargs) -> None:\n    duckdb_path = get_global_variables(kwargs['pipeline_uuid']).get('duckdb_path', '/home/src/duckdb/odoo_wh.duckdb')\n    Path(duckdb_path).parent.mkdir(parents=True, exist_ok=True)\n    upsert_to_duckdb(df, duckdb_path, table='fact_attendance', key='attendance_id', order_col='write_date')\n", "file_path": "data_exporters/x_fact_attendance_to_duckdb.py", "language": "python", "type": "data_exporter", "uuid": "x_fact_attendance_to_duckdb"}, "data_exporters/x_fact_leave_to_duckdb.py:data_exporter:python:x fact leave to duckdb": {"content": "# same helper as above\nimport duckdb, pandas as pd\nfrom pandas import DataFrame\nfrom pathlib import Path\n#from mage_ai.data_preparation.decorators import data_exporter\nfrom mage_ai.data_preparation.variable_manager import get_global_variables\n\n#print(\"[export] duckdb_version:\", duckdb.__version__)\n#print(\"[export] pragma_version:\", duckdb.connect().execute(\"PRAGMA version\").fetchall())\n\ndef upsert_to_duckdb(df: pd.DataFrame, path: str, table: str, key: str, order_col: str='write_date'):\n    if df is None or df.empty:\n        print(\"[export] DF kosong, skip\")\n        return\n\n    con = duckdb.connect(path)\n    try:\n        con.register('newdata', df)\n        con.execute(f\"CREATE TABLE IF NOT EXISTS {table} AS SELECT * FROM newdata WHERE 1=0;\")\n\n        cols = list(df.columns)\n        set_clause = ', '.join([f\"{c}=n.{c}\" for c in cols])\n        col_list  = ', '.join(cols)\n        val_list  = ', '.join([f\"n.{c}\" for c in cols])\n\n        con.execute(f\"\"\"\n            MERGE INTO {table} AS t\n            USING newdata AS n\n            ON t.{key} = n.{key}\n            WHEN MATCHED AND coalesce(n.{order_col}, TIMESTAMP '1970-01-01')\n                           >= coalesce(t.{order_col}, TIMESTAMP '1970-01-01')\n              THEN UPDATE SET {set_clause}\n            WHEN NOT MATCHED THEN\n              INSERT ({col_list}) VALUES ({val_list});\n        \"\"\")\n        print(\"[export] MERGE ok\")\n        con.unregister('newdata')\n    finally:\n        con.close()\n\n@data_exporter\ndef export_data(df: DataFrame, *args, **kwargs) -> None:\n    duckdb_path = get_global_variables(kwargs['pipeline_uuid']).get('duckdb_path', '/home/src/duckdb/odoo_wh.duckdb')\n    Path(duckdb_path).parent.mkdir(parents=True, exist_ok=True)\n    print(f\"[export] rows in df: {0 if df is None else len(df)}\")\n    upsert_to_duckdb(df, duckdb_path, table='fact_leave', key='leave_id', order_col='write_date')\n", "file_path": "data_exporters/x_fact_leave_to_duckdb.py", "language": "python", "type": "data_exporter", "uuid": "x_fact_leave_to_duckdb"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/psql_conn.py:data_loader:python:psql conn": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_postgres(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    query = 'select * from students'  # Specify your SQL query here\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        return loader.load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/psql_conn.py", "language": "python", "type": "data_loader", "uuid": "psql_conn"}, "data_loaders/load_hr_employee.sql:data_loader:sql:load hr employee": {"content": "WITH emp AS (\n  SELECT\n    e.id            AS employee_id,\n    e.name          AS employee_name,\n    e.work_email,\n    e.work_phone,\n    e.mobile_phone,\n    e.gender,\n    e.birthday,\n    e.active,\n    e.company_id,\n    e.department_id,\n    e.job_id,\n    e.create_date,\n    e.write_date\n  FROM hr_employee e\n  WHERE e.write_date >= COALESCE(TIMESTAMP '{{ variables.last_emp_ts | default(\"1970-01-01\") }}',\n                                 TIMESTAMP '1970-01-01')\n)\nSELECT\n  emp.*,\n  d.name AS department_name,\n  j.name AS job_name,\n  c.name AS company_name\nFROM emp\nLEFT JOIN hr_department d ON d.id = emp.department_id\nLEFT JOIN hr_job       j ON j.id = emp.job_id\nLEFT JOIN res_company  c ON c.id = emp.company_id\nORDER BY emp.write_date DESC, emp.employee_id DESC;\n", "file_path": "data_loaders/load_hr_employee.sql", "language": "sql", "type": "data_loader", "uuid": "load_hr_employee"}, "data_loaders/load_hr_attendance.sql:data_loader:sql:load hr attendance": {"content": "SELECT\n  a.id           AS attendance_id,\n  a.employee_id,\n  a.check_in,\n  a.check_out,\n  a.worked_hours,\n  a.create_date,\n  a.write_date\nFROM hr_attendance a\nWHERE a.write_date >= COALESCE(TIMESTAMP '{{ variables.last_att_ts | default(\"1970-01-01\") }}',\n                               TIMESTAMP '1970-01-01')\nORDER BY a.write_date DESC, a.id DESC;\n", "file_path": "data_loaders/load_hr_attendance.sql", "language": "sql", "type": "data_loader", "uuid": "load_hr_attendance"}, "data_loaders/load_hr_leave.sql:data_loader:sql:load hr leave": {"content": "SELECT\n  l.id                 AS leave_id,\n  l.employee_id,\n  l.holiday_status_id  AS leave_type_id,\n  l.request_date_from,\n  l.request_date_to,\n  l.number_of_days,\n  l.state,\n  l.create_date,\n  l.write_date\nFROM hr_leave l\nWHERE l.write_date >= COALESCE(TIMESTAMP '{{ variables.last_leave_ts | default(\"1970-01-01\") }}',\n                               TIMESTAMP '1970-01-01')\nORDER BY l.write_date DESC, l.id DESC;\n", "file_path": "data_loaders/load_hr_leave.sql", "language": "sql", "type": "data_loader", "uuid": "load_hr_leave"}, "data_loaders/cek_dw.sql:data_loader:sql:cek dw": {"content": "-- apakah tabel HR ada?\n--SELECT tablename\n--FROM pg_catalog.pg_tables\n--WHERE schemaname = 'public'\n--AND tablename IN ('hr_employee','hr_attendance','hr_leave');\n\n-- hitung baris (akan error kalau modul belum terpasang)\n--SELECT count(*) FROM hr_employee;\n--SELECT count(*) FROM hr_attendance;\nSELECT count(*) FROM hr_leave;\n", "file_path": "data_loaders/cek_dw.sql", "language": "sql", "type": "data_loader", "uuid": "cek_dw"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/t_clean_employee.py:transformer:python:t clean employee": {"content": "import pandas as pd\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    df = df.copy()\n    # trim string\n    for c in ['employee_name','work_email','work_phone','mobile_phone','gender',\n              'department_name','job_name','company_name']:\n        if c in df.columns:\n            df[c] = df[c].astype('string').str.strip()\n    # parse waktu \u2192 UTC\n    for c in ['birthday','create_date','write_date']:\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], utc=True, errors='coerce')\n    if 'active' in df.columns:\n        df['active'] = df['active'].astype('boolean')\n    # kolom housekeeping\n    df['_ingested_at'] = pd.Timestamp.now(tz='UTC')\n    return df\n", "file_path": "transformers/t_clean_employee.py", "language": "python", "type": "transformer", "uuid": "t_clean_employee"}, "transformers/t_clean_attendance.py:transformer:python:t clean attendance": {"content": "import pandas as pd\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    df = df.copy()\n    for c in ['check_in','check_out','create_date','write_date']:\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], utc=True, errors='coerce')\n    df['_ingested_at'] = pd.Timestamp.now(tz='UTC')\n    return df\n", "file_path": "transformers/t_clean_attendance.py", "language": "python", "type": "transformer", "uuid": "t_clean_attendance"}, "transformers/t_clean_leave.py:transformer:python:t clean leave": {"content": "#from mage_ai.data_preparation.decorators import transformer\nimport pandas as pd\n\n@transformer\ndef transform(df, *args, **kwargs):\n    df = df.copy()\n    for c in ('request_date_from', 'request_date_to', 'create_date', 'write_date'):\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], utc=True, errors='coerce')\n    df['_ingested_at'] = pd.Timestamp.now(tz='UTC')\n    return df\n", "file_path": "transformers/t_clean_leave.py", "language": "python", "type": "transformer", "uuid": "t_clean_leave"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/get_postgres/__init__.py:pipeline:python:get postgres/  init  ": {"content": "", "file_path": "pipelines/get_postgres/__init__.py", "language": "python", "type": "pipeline", "uuid": "get_postgres/__init__"}, "pipelines/get_postgres/metadata.yaml:pipeline:yaml:get postgres/metadata": {"content": "blocks: []\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-24 03:36:12.394412+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: get postgres\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: get_postgres\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/get_postgres/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "get_postgres/metadata"}, "pipelines/odoo_hr_to_duckdb/__init__.py:pipeline:python:odoo hr to duckdb/  init  ": {"content": "", "file_path": "pipelines/odoo_hr_to_duckdb/__init__.py", "language": "python", "type": "pipeline", "uuid": "odoo_hr_to_duckdb/__init__"}, "pipelines/odoo_hr_to_duckdb/metadata.yaml:pipeline:yaml:odoo hr to duckdb/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    data_provider: postgres\n    data_provider_profile: default\n    dbt: {}\n    disable_query_preprocessing: false\n    export_write_policy: append\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks:\n  - t_clean_employee\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: load_hr_employee\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_hr_employee\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    data_provider: postgres\n    data_provider_profile: default\n    dbt: {}\n    disable_query_preprocessing: false\n    export_write_policy: append\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks:\n  - t_clean_attendance\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: load_hr_attendance\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_hr_attendance\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    data_provider: postgres\n    data_provider_profile: default\n    dbt: {}\n    disable_query_preprocessing: false\n    export_write_policy: append\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks:\n  - t_clean_leave\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: load_hr_leave\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_hr_leave\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - x_dim_employee_to_duckdb\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: t_clean_employee\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_hr_employee\n  uuid: t_clean_employee\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - x_fact_attendance_to_duckdb\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: t_clean_attendance\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_hr_attendance\n  uuid: t_clean_attendance\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - x_fact_leave_to_duckdb\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: t_clean_leave\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_hr_leave\n  uuid: t_clean_leave\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/x_dim_employee_to_duckdb.py\n    file_source:\n      path: data_exporters/x_dim_employee_to_duckdb.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: x_dim_employee_to_duckdb\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - t_clean_employee\n  uuid: x_dim_employee_to_duckdb\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: x_fact_attendance_to_duckdb\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - t_clean_attendance\n  uuid: x_fact_attendance_to_duckdb\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: x_fact_leave_to_duckdb\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - t_clean_leave\n  uuid: x_fact_leave_to_duckdb\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-24 05:58:27.607156+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: odoo_hr_to_duckdb\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: odoo_hr_to_duckdb\nvariables:\n  duckdb_path: /home/src/duckdb/odoo_wh.duckdb\n  last_att_ts: '1970-01-01'\n  last_emp_ts: '1970-01-01'\n  last_leave_ts: '1970-01-01'\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/odoo_hr_to_duckdb/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "odoo_hr_to_duckdb/metadata"}, "/home/src/default_repo/data_exporters/fact_leave_ch.py:data_exporter:python:home/src/default repo/data exporters/fact leave ch": {"content": "#from mage_ai.data_preparation.decorators import data_exporter\nimport clickhouse_connect\nimport pandas as pd\n\n@data_exporter\ndef export_data(df: pd.DataFrame, *args, **kwargs) -> None:\n    if df is None or df.empty:\n        print(\"[export] DF kosong, skip\"); return\n\n    # Bersihkan timezone: ClickHouse DateTime (tanpa tz) \u2192 kirim naive UTC\n    for c in ['request_date_from','request_date_to','create_date','write_date','_ingested_at']:\n        if c in df.columns:\n            s = pd.to_datetime(df[c], utc=True, errors='coerce')\n            df[c] = s.dt.tz_convert('UTC').dt.tz_localize(None)\n\n    client = clickhouse_connect.get_client(\n        host='clickhouse', port=8123,\n        username='odoo', password='odooadm', database='odoo_dwh'\n    )\n\n    client.query(\"\"\"\n        CREATE TABLE IF NOT EXISTS fact_leave\n        (\n          leave_id          UInt64,\n          employee_id       UInt64,\n          leave_type_id     Nullable(UInt64),     -- <- buat Nullable kalau bisa NULL\n          request_date_from Nullable(DateTime),\n          request_date_to   Nullable(DateTime),\n          number_of_days    Nullable(Float32),\n          state             LowCardinality(String),\n          create_date       Nullable(DateTime),\n          write_date        DateTime,\n          _ingested_at      DateTime\n        )\n        ENGINE = ReplacingMergeTree(write_date)\n        ORDER BY (leave_id)\n    \"\"\")\n\n    # Ini yang penting: gunakan insert_df, bukan insert(list-of-dicts)\n    client.insert_df('fact_leave', df)\n    print(f\"[export] inserted rows: {len(df)}\")\n", "file_path": "/home/src/default_repo/data_exporters/fact_leave_ch.py", "language": "python", "type": "data_exporter", "uuid": "fact_leave_ch"}, "/home/src/default_repo/data_exporters/fact_attendance_ch.py:data_exporter:python:home/src/default repo/data exporters/fact attendance ch": {"content": "#from mage_ai.data_preparation.decorators import data_exporter\nimport clickhouse_connect\nimport pandas as pd\n\ndef _to_naive_utc(series):\n    s = pd.to_datetime(series, utc=True, errors='coerce')\n    return s.dt.tz_localize(None)\n\n@data_exporter\ndef export_data(df: pd.DataFrame, *args, **kwargs) -> None:\n    if df is None or df.empty:\n        print(\"[export fact_attendance] DF kosong, skip\"); return\n\n    # --- Normalisasi waktu ke naive UTC ---\n    for c in ['check_in','check_out','create_date','write_date','_ingested_at']:\n        if c in df.columns:\n            df[c] = _to_naive_utc(df[c])\n\n    # worked_hours bisa float / NaN\n    if 'worked_hours' in df.columns:\n        df['worked_hours'] = pd.to_numeric(df['worked_hours'], errors='coerce')\n\n    # --- Koneksi ClickHouse ---\n    client = clickhouse_connect.get_client(\n        host='clickhouse', port=8123,\n        username='odoo', password='odooadm', database='odoo_dwh'\n    )\n\n    # --- Buat tabel jika belum ada ---\n    client.query(\"\"\"\n        CREATE TABLE IF NOT EXISTS fact_attendance\n        (\n          attendance_id   UInt64,\n          employee_id     UInt64,\n          check_in        Nullable(DateTime),\n          check_out       Nullable(DateTime),\n          worked_hours    Nullable(Float32),\n          create_date     Nullable(DateTime),\n          write_date      DateTime,\n          _ingested_at    DateTime\n        )\n        ENGINE = ReplacingMergeTree(write_date)\n        ORDER BY (attendance_id)\n    \"\"\")\n\n    # Isi write_date jika kosong\n    if 'write_date' in df.columns and '_ingested_at' in df.columns:\n        df['write_date'] = df['write_date'].fillna(df['_ingested_at'])\n\n    # --- Insert ---\n    client.insert_df('fact_attendance', df)\n    print(f\"[export fact_attendance] inserted rows: {len(df)}\")\n", "file_path": "/home/src/default_repo/data_exporters/fact_attendance_ch.py", "language": "python", "type": "data_exporter", "uuid": "fact_attendance_ch"}, "/home/src/default_repo/data_exporters/dim_employee_ch.py:data_exporter:python:home/src/default repo/data exporters/dim employee ch": {"content": "#from mage_ai.data_preparation.decorators import data_exporter\nimport clickhouse_connect\nimport pandas as pd\n\ndef _to_naive_utc(series):\n    s = pd.to_datetime(series, utc=True, errors='coerce')\n    return s.dt.tz_localize(None)\n\n@data_exporter\ndef export_data(df: pd.DataFrame, *args, **kwargs) -> None:\n    if df is None or df.empty:\n        print(\"[export dim_employee] DF kosong, skip\"); return\n\n    # --- Normalisasi kolom waktu ---\n    for c in ['create_date', 'write_date', '_ingested_at']:\n        if c in df.columns:\n            df[c] = _to_naive_utc(df[c])\n\n    # birthday -> Date (bukan DateTime)\n    if 'birthday' in df.columns:\n        s = pd.to_datetime(df['birthday'], utc=True, errors='coerce')\n        df['birthday'] = s.dt.tz_localize(None).dt.date\n\n    # active -> UInt8 (1/0/NULL)\n    if 'active' in df.columns:\n        b = df['active'].astype('boolean')\n        df['active'] = b.apply(lambda x: 1 if x is True else (0 if x is False else None))\n\n    # Pastikan kolom string ada (kalau tidak, buat NULL)\n    for col in ['company_name','department_name','job_name','work_email','work_phone','mobile_phone','gender','employee_name']:\n        if col not in df.columns:\n            df[col] = None\n\n    # Isi write_date jika kosong (pakai _ingested_at)  <-- perbaikan: tanpa spasi nyasar\n    if 'write_date' in df.columns and '_ingested_at' in df.columns:\n        df['write_date'] = df['write_date'].fillna(df['_ingested_at'])\n\n    client = clickhouse_connect.get_client(\n        host='clickhouse', port=8123,\n        username='odoo', password='odooadm', database='odoo_dwh'\n    )\n\n    # DDL: gunakan LowCardinality(Nullable(String)), BUKAN Nullable(LowCardinality(String))\n    client.query(\"\"\"\n        CREATE TABLE IF NOT EXISTS dim_employee\n        (\n          employee_id        UInt64,\n          employee_name      LowCardinality(Nullable(String)),\n          work_email         LowCardinality(Nullable(String)),\n          work_phone         Nullable(String),\n          mobile_phone       Nullable(String),\n          gender             LowCardinality(Nullable(String)),\n          birthday           Nullable(Date),\n          active             Nullable(UInt8),\n          company_id         Nullable(UInt64),\n          company_name       LowCardinality(Nullable(String)),\n          department_id      Nullable(UInt64),\n          department_name    LowCardinality(Nullable(String)),\n          job_id             Nullable(UInt64),\n          job_name           LowCardinality(Nullable(String)),\n          create_date        Nullable(DateTime),\n          write_date         DateTime,\n          _ingested_at       DateTime\n        )\n        ENGINE = ReplacingMergeTree(write_date)\n        ORDER BY (employee_id)\n    \"\"\")\n\n    client.insert_df('dim_employee', df)\n    print(f\"[export dim_employee] inserted rows: {len(df)}\")\n", "file_path": "/home/src/default_repo/data_exporters/dim_employee_ch.py", "language": "python", "type": "data_exporter", "uuid": "dim_employee_ch"}, "/home/src/default_repo/transformers/t_clean_employee.py:transformer:python:home/src/default repo/transformers/t clean employee": {"content": "import pandas as pd\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    df = df.copy()\n    # trim string\n    for c in ['employee_name','work_email','work_phone','mobile_phone','gender',\n              'department_name','job_name','company_name']:\n        if c in df.columns:\n            df[c] = df[c].astype('string').str.strip()\n    # parse waktu \u2192 UTC\n    for c in ['birthday','create_date','write_date']:\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], utc=True, errors='coerce')\n    if 'active' in df.columns:\n        df['active'] = df['active'].astype('boolean')\n    # kolom housekeeping\n    df['_ingested_at'] = pd.Timestamp.now(tz='UTC')\n    return df\n", "file_path": "/home/src/default_repo/transformers/t_clean_employee.py", "language": "python", "type": "transformer", "uuid": "t_clean_employee"}, "/home/src/default_repo/transformers/t_clean_attendance.py:transformer:python:home/src/default repo/transformers/t clean attendance": {"content": "import pandas as pd\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    df = df.copy()\n    for c in ['check_in','check_out','create_date','write_date']:\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], utc=True, errors='coerce')\n    df['_ingested_at'] = pd.Timestamp.now(tz='UTC')\n    return df\n", "file_path": "/home/src/default_repo/transformers/t_clean_attendance.py", "language": "python", "type": "transformer", "uuid": "t_clean_attendance"}, "/home/src/default_repo/transformers/t_clean_leave.py:transformer:python:home/src/default repo/transformers/t clean leave": {"content": "#from mage_ai.data_preparation.decorators import transformer\nimport pandas as pd\n\n@transformer\ndef transform(df, *args, **kwargs):\n    df = df.copy()\n    for c in ('request_date_from', 'request_date_to', 'create_date', 'write_date'):\n        if c in df.columns:\n            df[c] = pd.to_datetime(df[c], utc=True, errors='coerce')\n    df['_ingested_at'] = pd.Timestamp.now(tz='UTC')\n    return df\n", "file_path": "/home/src/default_repo/transformers/t_clean_leave.py", "language": "python", "type": "transformer", "uuid": "t_clean_leave"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}